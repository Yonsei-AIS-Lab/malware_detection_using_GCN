import torch
import torch.nn as nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import DataLoader
import torch.nn.functional as F

class GCN(torch.nn.Module):
    def __init__(self, nb_features, nb_classes):
        super().__init__()
        self.conv1 = GCNConv(nb_features, 64)
        self.conv2 = GCNConv(64, nb_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)

        return F.log_softmax(x, dim=1)
    
def train(model, train, test, epochs):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)


    log = {}
    log['train_loss'] = []
    log['train_acc'] = []
    log['test_loss'] = []
    log['test_acc'] = []

    for epoch in range(epochs):
        train_cnt = 0
        train_loss = 0
        train_acc = 0

        model.train()
        optimizer.zero_grad()

        for data in train:
            data = data.to(device)
            target = data.y.to(device)

            output = model(data)

            pred = torch.argmax(output, dim=1)
            correct = torch.eq(pred, target).sum().item()
            acc = correct / len(target)

            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()

            train_loss += loss
            train_acc += acc
            train_cnt += 1

        train_loss /= train_cnt
        train_acc /= train_cnt

        test_cnt = 0
        test_loss = 0
        test_acc = 0

        model.eval()
        with torch.no_grad():
            for data in test:
                data = data.to(device)
                target = data.y.to(device)

                output = model(data)

                pred = torch.argmax(output, dim=1)
                correct = torch.eq(pred, target).sum().item()
                acc = correct / len(target)

                loss = F.nll_loss(output, target)

                test_loss += loss
                test_acc += acc
                test_cnt += 1
            test_loss /= test_cnt            
            test_acc /= test_cnt

        log['train_loss'].append(train_loss.detach().item())
        log['train_acc'].append(train_acc)
        log['test_loss'].append(test_loss.detach().item())
        log['test_acc'].append(test_acc)

        print(f'{epoch+1:>3} Epochs: t_loss[{train_loss:.2f}], t_acc[{train_acc*100:.2f} %]  /  loss[{test_loss:.2f}], acc[{test_acc*100:.2f} %]')

    return log