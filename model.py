import torch
import torch.nn as nn
from torch.nn import Linear, BatchNorm1d
from torch_geometric.nn import GCNConv
from torch_geometric.loader import DataLoader
import torch.nn.functional as F

class GCN(torch.nn.Module):
    def __init__(self, nb_features, nb_classes, nb_layers, hidden_size):
        super().__init__()

        self.layers = torch.nn.ModuleList([])
        for i in range(nb_layers):
            if i == 0:
                self.layers.append(GCNConv(nb_features, hidden_size))
            else:
                self.layers.append(GCNConv(hidden_size, hidden_size))
            #self.layers.append(nn.BatchNorm1d(hidden_size))
        self.conv_out = GCNConv(hidden_size, nb_classes)


    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        for layer in self.layers:
            if isinstance(layer, GCNConv):
                x = layer(x, edge_index)
                x = F.relu(x)
            elif isinstance(layer, nn.BatchNorm1d):
                x = layer(x)

        x = self.conv_out(x, edge_index)

        return F.log_softmax(x, dim=1)
    
def train(device, model, optimizer, data_loader):
    train_cnt = 0
    train_loss = 0
    train_acc = 0

    nb_unused_nodes = 0
    nb_all_nodes = 0

    model.train()
    optimizer.zero_grad()

    for data in data_loader:
        data = data.to(device)
        target = data.y.to(device)

        output = model(data)

        # unused node removal
        unuse_indices = torch.nonzero(target != -1)

        nb_unused_nodes += len(unuse_indices)
        nb_all_nodes += len(target)

        target = target[unuse_indices].squeeze()
        output = output[unuse_indices]
        output = output.view(output.shape[0], output.shape[2])

        pred = torch.argmax(output, dim=1)
        correct = torch.eq(pred, target).sum().item()
        acc = correct / len(target)

        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()

        train_loss += loss
        train_acc += acc
        train_cnt += 1

    train_loss /= train_cnt
    train_acc /= train_cnt
    
    return train_loss.detach().item(), train_acc

def validation(device, model, data_loader):
    test_cnt = 0
    test_loss = 0
    test_acc = 0

    model.eval()
    with torch.no_grad():
        for data in data_loader:
            data = data.to(device)
            target = data.y.to(device)

            output = model(data)

            # unused node removal
            unuse_indices = torch.nonzero(target != -1)
            target = target[unuse_indices].squeeze()
            output = output[unuse_indices]
            output = output.view(output.shape[0], output.shape[2])

            pred = torch.argmax(output, dim=1)
            correct = torch.eq(pred, target).sum().item()
            acc = correct / len(target)

            loss = F.nll_loss(output, target)

            test_loss += loss
            test_acc += acc
            test_cnt += 1
        test_loss /= test_cnt            
        test_acc /= test_cnt

    return test_loss, test_acc