{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(cur, total):\n",
    "    print(f\"## Progress: {cur+1}/{total} ##\", end='\\r')\n",
    "\n",
    "def get_data_info(data):\n",
    "    \"\"\"\n",
    "    0: Normal\n",
    "    1: Extortion Virus\n",
    "    2: Mining Program\n",
    "    3: DDoS Trojan Horse\n",
    "    4: Worm Virus\n",
    "    5: Infectious Virus\n",
    "    6: Backdoor Program\n",
    "    7: Trojan horse Program\n",
    "    \"\"\"\n",
    "    label = data.groupby(['file_id', 'label'])['label'].unique()\n",
    "    counts = label.value_counts()\n",
    "    apis = data['api'].unique()\n",
    "    files = data['file_id'].unique()\n",
    "    \n",
    "    nb_files = len(files)\n",
    "    nb_data = len(data)\n",
    "    nb_labels = len(counts)\n",
    "    nb_apis = len(apis)+1\n",
    "\n",
    "    print('-----| Get Data Information |-----')\n",
    "    print(f\"- Files    : {nb_files:<6}\\n\"\n",
    "          f\"- Data     : {nb_data:<6}\\n\"\n",
    "          f\"- Labels   : {nb_labels:<6}\\n\"\n",
    "          f\"- APIs     : {nb_apis:<6}\\n\"\n",
    "          )\n",
    "\n",
    "    return nb_files, nb_data, nb_labels, nb_apis\n",
    "\n",
    "def split_per_id(data, nb_files):\n",
    "    split_per_flie_id = []\n",
    "    print('-----| Split Per File ID |-----')\n",
    "    for i in range(nb_files):\n",
    "        split_per_flie_id.append(data[data['file_id']==i+1])\n",
    "        print_progress(i, nb_files)\n",
    "    print('\\n')\n",
    "\n",
    "    return split_per_flie_id\n",
    "\n",
    "def creat_api2idx(data):\n",
    "    api_names = data['api'].unique()\n",
    "\n",
    "    api2idx = {}\n",
    "    api2idx['[UNK]'] = 0\n",
    "    for i, api_name in enumerate(api_names):\n",
    "        api2idx[api_name] = i+1\n",
    "\n",
    "    return api2idx\n",
    "\n",
    "def encode_api_name(data_split_per_id, api2idx):\n",
    "    file_len = len(data_split_per_id)\n",
    "    files = []\n",
    "    labels = []\n",
    "    print('-----| Encode API Name |-----')\n",
    "\n",
    "    for i, datas in enumerate(data_split_per_id):\n",
    "        encoded_data = datas.copy()\n",
    "        encoded_apis = (encoded_data['api'].map(api2idx)).fillna(0).astype(int).to_list()\n",
    "        label = encoded_data['label'].to_list()[0]\n",
    "\n",
    "        files.append(encoded_apis)\n",
    "        labels.append(label)\n",
    "\n",
    "        print_progress(i, file_len)\n",
    "    print('\\n')\n",
    "    \n",
    "    return files, np.array(labels)\n",
    "\n",
    "def create_edge_index(files):\n",
    "    print('-----| Create Edge Index |-----')\n",
    "    file_len = len(files)\n",
    "\n",
    "    results = []\n",
    "    for i, nodes in enumerate(files):\n",
    "        src, dst = nodes[:-1], nodes[1:]\n",
    "\n",
    "        edge_idx = np.array([src, dst])\n",
    "\n",
    "        results.append(edge_idx)\n",
    "        print_progress(i, file_len)\n",
    "    print('\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_adj(nb_feature, srcs, dsts):\n",
    "    adj = np.zeros((nb_feature, nb_feature))\n",
    "    for i in range(len(srcs)):\n",
    "        adj[srcs[i], dsts[i]] += 1\n",
    "    return adj\n",
    "\n",
    "def norm_adj(adj_mat, f_type):\n",
    "    if f_type=='Normal':\n",
    "        a0 = 0.5 * (adj_mat + adj_mat.T)\n",
    "        max_value = np.max(a0)\n",
    "        if max_value == 0:\n",
    "            max_value = 1\n",
    "        adj_mat = a0 / max_value\n",
    "    \n",
    "    elif f_type=='Aggregation':\n",
    "        sum_adj = adj_mat.sum(axis=0, keepdims=True)\n",
    "        sum_adj[sum_adj == 0] = 1\n",
    "        a1 = adj_mat / sum_adj\n",
    "        max_value = np.max(a1)\n",
    "        if max_value == 0:\n",
    "            max_value = 1\n",
    "        adj_mat = a1 / max_value\n",
    "\n",
    "    elif f_type=='Propagation':\n",
    "        sum_adj = adj_mat.sum(axis=0, keepdims=True)\n",
    "        sum_adj[sum_adj == 0] = 1\n",
    "        a2 = adj_mat / sum_adj            \n",
    "        a02 = 0.5 * (a2 + a2.T)\n",
    "        max_value = np.max(a02)\n",
    "        if max_value == 0:\n",
    "            max_value = 1\n",
    "        adj_mat = a02 / max_value\n",
    "    \n",
    "    return adj_mat\n",
    "\n",
    "def get_normalized_adj(nb_apis, edge_idx):\n",
    "    print('-----| Create Adjacency Matrix |-----')\n",
    "\n",
    "    file_len = len(edge_idx)\n",
    "    adjs_origin = []\n",
    "    adjs = []\n",
    "    for i, edges in enumerate(edge_idx):\n",
    "        src = edges[0]\n",
    "        dst = edges[1]\n",
    "\n",
    "        adj = get_adj(nb_apis, src, dst)\n",
    "\n",
    "        nb_adj = 3\n",
    "        adj_norm = []\n",
    "        adj_norm.append(norm_adj(adj, 'Normal'))\n",
    "        adj_norm.append(norm_adj(adj, 'Aggregation'))\n",
    "        adj_norm.append(norm_adj(adj, 'Propagation'))\n",
    "\n",
    "        adj_norm = np.array(adj_norm)\n",
    "        adj_norm = adj_norm.reshape((nb_adj, adj.shape[0], adj.shape[1]))\n",
    "\n",
    "        adjs_origin.append(adj)\n",
    "        adjs.append(adj_norm)\n",
    "        print_progress(i, file_len)\n",
    "    print('\\n')\n",
    "\n",
    "    return np.array(adjs_origin), np.array(adjs)\n",
    "\n",
    "def create_feature_mat(nb_apis, edge_idx):\n",
    "    print('-----| Create Feature Matrix |-----')\n",
    "\n",
    "    file_len = len(edge_idx)\n",
    "    nb_features = 12\n",
    "    features = []\n",
    "\n",
    "    for i, edge in enumerate(edge_idx):\n",
    "        feature = np.zeros((nb_apis, nb_features))\n",
    "\n",
    "        if edge.size == 0:\n",
    "            features.append(feature)\n",
    "            continue\n",
    "        \n",
    "        sequence = np.insert(edge[1], 0, edge[0][0])\n",
    "        src = edge[0]\n",
    "        dst = edge[1]\n",
    "\n",
    "        # add amount number of each API\n",
    "        api_num, counts = np.unique(sequence, return_counts=True)\n",
    "        for j, api_idx in enumerate(api_num):\n",
    "            # add counts\n",
    "            feature[api_idx][0] += counts[j]\n",
    "\n",
    "            # add first appear\n",
    "            feature[api_idx][1] = np.where(sequence == api_idx)[0][0]\n",
    "\n",
    "            # add last appear\n",
    "            feature[api_idx][2] = np.where(sequence == api_idx)[0][-1]\n",
    "\n",
    "        # add in degree\n",
    "        in_apis, counts = np.unique(src, return_counts=True)\n",
    "        for j, api_idx in enumerate(in_apis):\n",
    "            feature[api_idx][4] = counts[j]\n",
    "\n",
    "        for j, api_idx in enumerate(in_apis):\n",
    "            neighborhoods = np.unique(src[np.where(dst == api_idx)])\n",
    "            in_mean = 0\n",
    "            in_min = 1000\n",
    "            in_max = 0\n",
    "            in_cnt = 0        \n",
    "            for nei_idx in neighborhoods:\n",
    "                cur_deg = feature[nei_idx][4]\n",
    "                if in_min > cur_deg:\n",
    "                    in_min = cur_deg\n",
    "                if in_max < cur_deg:\n",
    "                    in_max = cur_deg\n",
    "                in_mean += cur_deg\n",
    "                in_cnt += 1\n",
    "            if in_cnt == 0:\n",
    "                in_mean = 0\n",
    "            else:\n",
    "                in_mean /= in_cnt\n",
    "\n",
    "            feature[api_idx][5] = in_min\n",
    "            feature[api_idx][6] = in_max\n",
    "            feature[api_idx][7] = in_mean\n",
    "\n",
    "        # add out degree\n",
    "        out_apis, counts = np.unique(dst, return_counts=True)\n",
    "        for j, api_idx in enumerate(out_apis):\n",
    "            feature[api_idx][8] = counts[j]\n",
    "\n",
    "        for j, api_idx in enumerate(in_apis):\n",
    "            neighborhoods = np.unique(src[np.where(src == api_idx)])\n",
    "            out_mean = 0\n",
    "            out_min = 1000\n",
    "            out_max = 0\n",
    "            out_cnt = 0        \n",
    "            for nei_idx in neighborhoods:\n",
    "                cur_deg = feature[nei_idx][8]\n",
    "                if out_min > cur_deg:\n",
    "                    out_min = cur_deg\n",
    "                if out_max < cur_deg:\n",
    "                    out_max = cur_deg\n",
    "                out_mean += cur_deg\n",
    "                out_cnt += 1\n",
    "            if out_cnt == 0:\n",
    "                out_mean = 0\n",
    "            else:\n",
    "                out_mean /= out_cnt\n",
    "\n",
    "            feature[api_idx][9] = out_min\n",
    "            feature[api_idx][10] = out_max\n",
    "            feature[api_idx][11] = out_mean\n",
    "\n",
    "        features.append(feature)\n",
    "        print_progress(i, file_len)\n",
    "    print('\\n')\n",
    "\n",
    "    return np.array(features), nb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----| Get Data Information |-----\n",
      "- Files    : 1000  \n",
      "- Data     : 6308957\n",
      "- Labels   : 8     \n",
      "- APIs     : 270   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = './datasets/security_train_sliced_1000.csv'\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "nb_files, nb_data, nb_labels, nb_apis = get_data_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----| Split Per File ID |-----\n",
      "## Progress: 1000/1000 ##\n",
      "\n",
      "-----| Encode API Name |-----\n",
      "## Progress: 1000/1000 ##\n",
      "\n",
      "-----| Create Edge Index |-----\n",
      "## Progress: 1000/1000 ##\n",
      "\n",
      "-----| Create Adjacency Matrix |-----\n",
      "## Progress: 1000/1000 ##\n",
      "\n",
      "-----| Create Feature Matrix |-----\n",
      "## Progress: 1000/1000 ##\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_split_per_id = split_per_id(data, nb_files)\n",
    "api2idx = creat_api2idx(data)\n",
    "files, labels = encode_api_name(data_split_per_id, api2idx)\n",
    "edge_idx = create_edge_index(files)\n",
    "adjs_origin, adjs_normalized = get_normalized_adj(nb_apis, edge_idx)\n",
    "feature_matrix, nb_features = create_feature_mat(nb_apis, edge_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlibabaDataset(Dataset):\n",
    "    def __init__(self, adj, feature, label):\n",
    "        self.adj = adj\n",
    "        self.feature = feature\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        adj = self.adj[idx]\n",
    "        feature = self.feature[idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        return adj, feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "x_adj_train, x_adj_test, y_train, y_test = train_test_split(adjs_normalized, labels, test_size=0.1, random_state=1019, shuffle=False)\n",
    "x_feat_train, x_feat_test = train_test_split(feature_matrix, test_size=0.1, random_state=1019, shuffle=False)\n",
    "\n",
    "train_dataset = AlibabaDataset(x_adj_train, x_feat_train, y_train)\n",
    "test_dataset = AlibabaDataset(x_adj_test, x_feat_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nb_nodes, nb_features, nb_norm_adj):\n",
    "        super(GCN, self).__init__()\n",
    "        self.w_z0 = nn.Parameter(torch.randn(nb_nodes, nb_nodes), requires_grad=True)\n",
    "        self.w_z1 = nn.Parameter(torch.randn(nb_nodes, nb_nodes), requires_grad=True)\n",
    "        self.w_z2 = nn.Parameter(torch.randn(nb_nodes, nb_nodes), requires_grad=True)\n",
    "\n",
    "        print(self)\n",
    "        \n",
    "    def forward(self, x_adj):\n",
    "        z0 = torch.matmul(x_adj[:, 0], self.w_z0)\n",
    "        z1 = torch.matmul(x_adj[:, 1], self.w_z1)\n",
    "        z2 = torch.matmul(x_adj[:, 2], self.w_z2)\n",
    "        \n",
    "        z = z0 + z1 + z2\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN()\n"
     ]
    }
   ],
   "source": [
    "model = GCN(nb_apis, nb_features, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = torch.tensor(x_adj_train[:10], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 270, 270])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 270, 270])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
